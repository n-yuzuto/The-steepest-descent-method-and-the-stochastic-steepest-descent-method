# The-steepest-descent-method-and-the-stochastic-steepest-descent-method
Wineデータを訓練事例とテスト事例に分割し、訓練事例でridge回帰を勾配降下法(GD)と確率的勾配降下法(SGD)によって最適化した。 いずれも複数のステップサイズパラメータetaで実行した。

## 考察
### ① GDではステップサイズにかかわらすアルゴリズム停止時にはほぼ同じ誤差を達成しているが、ステップサイズによって停止までにかかるステップ数が異なる。  
理由： 一回更新あたりのパラメータの変化量はステップサイズによって決まるため、一般的には、ステップサイズが小さい方が、一回の更新量が少なく、最適解付近に到達するまで時間がかかるため.  

### ② GDでは目的関数が単調減少するのに対して、SGDではそうならない  
理由： 目的関数は全データの二乗誤差和と正則化の和である。GDではこの目的関数について勾配方向にパラメータを更新しているため、1ステップごとに必ず目的関数値が減少する。一方、SGDでは、単一のデータについてのみ勾配を計算するため、更新の方向は必ずしも目的関数(誤差)を減少させるとは限らない。長期的には目的関数は減少することが期待できるが、1ステップごとには必ずしも目的関数値は減少しない。

SGDは一回の更新で必ずしも目的関数が減少しないため、GDと同じように収束判定を行うと、アルゴリズムがすぐに停止してしまう。目的関数が過去の最小の誤差に対して、T回連続で改善されなければ停止する、という方法が最も簡単な収束判定法である。


***
.ipynbファイルが開かれない時は、こちらのリンクにURLを貼ってご覧になってください。  
[nbviewer](https://nbviewer.jupyter.org/)
